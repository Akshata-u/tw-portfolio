{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Technical Writing Portfolio","text":"<p>Hi I'm Akshata, a technical writer who turns complex products into clear, shippable, and genuinely useful docs. I partner closely with Product, Design and Engineering teams; and also collaborate with Support and GTM teams so documentation launches with the feature, not after it!</p> <p>Business outcomes I achieve: faster onboarding (internal and external), fewer tickets, and higher adoption. My areas of documentation expertise span UI and API docs, in-app guidance (walkthroughs/ tooltips), release notes, and enablement assets; underpinned by appropriate information architecture, versioning (where required), and  review workflows.</p> <p>I can design a scalable content system and also write the words, examples, and visuals that help users succeed. My site houses a collection of various types of documentation to give you an idea of my writing style and skills.</p> Explore Now! My Process Tools <p></p>"},{"location":"api-integration/","title":"API & Integration documentation samples","text":""},{"location":"api-integration/#apis","title":"APIs","text":"<p>Group-image API \u2192 </p> <p>Asset Management API \u2192 </p>"},{"location":"api-integration/#integration","title":"Integration","text":"<p>Nice inContact &lt;&gt; Instance Integration \u2192 </p> <p>Data Export Snowflake instance to SW3 \u2192</p> <p></p>"},{"location":"api-integration/api-air-1/","title":"Group-image API","text":"<p>About this article </p> <p>Reference links to other articles are indicated but unavailable since these are sample documents.</p>"},{"location":"api-integration/api-air-1/#api-short-description","title":"API Short Description","text":"<p>The Group-image API will segregate a given batch of assorted images into groups based on their contextual similarity i.e., likeness of the theme, color composition, focus objects, etc. in each image. You can define the number of groups that the API should generate, as well as the minimum and maximum number of images in each group.</p> <p>The accepted input image formats are PNG and JPEG. Currently, SVG format is not supported.</p>"},{"location":"api-integration/api-air-1/#use-case","title":"Use Case","text":"<p>For products such as photobooks, calendars, etc. where customers can upload multiple images, the Group-image API can be used to suggest arrangements of similar images on each page of the product; making it aesthetically and visually more appealing.</p> <p></p>"},{"location":"api-integration/api-air-1/#usage-example","title":"Usage Example","text":"<p>The Group-image API groups the input images based on their visual similarity. In the example below, the API was instructed to greate a maximum of 2 groups.</p> <p></p>"},{"location":"api-integration/api-air-1/#api-parameters","title":"API Parameters","text":""},{"location":"api-integration/api-air-1/#input","title":"Input","text":"<p>Try it out!</p> <p>For the latest details and to try out this API, visit our Swagger page.</p> <p>Input object <pre><code>{\n  \"input\": {\n    \"urls\": [\n      \"string\"\n    ],\n    \"totalGroups\": 0,\n    \"minImagesPerGroup\": 0,\n    \"maxImagesPerGroup\": 0\n  },\n  \"callbackUrl\": \"string\"\n}\n</code></pre></p> <p><code>urls</code> - string - (Required)</p> <ul> <li>The list of URLs of the input images.</li> </ul> <p><code>totalGroups</code>: number - (Required)</p> <ul> <li>The number of image groups that the API should create.</li> </ul> <p><code>minImagesPerGroup</code>: number - (Optional. Default:<code>1</code>)</p> <ul> <li>The minimum number of images that a group should contain.</li> </ul> <p><code>maxImagesPerGroup</code>: number - (Optional)</p> <ul> <li>The maximum number of images a group can contain. If you do not specify any value, the API will automatically calculate the value.</li> </ul> <p><code>callbackUrl</code>: string - (Optional)</p> <ul> <li>If you specify this parameter, a callback will be made when the operation is completed. Please refer to this link for more information.</li> </ul>"},{"location":"api-integration/api-air-1/#output","title":"Output","text":"<pre><code>{\n  \"id\": \"string\",\n  \"status\": \"string\",\n  \"callbackUrl\": \"string\",\n  \"input\": {\n    \"urls\": [\n      \"string\"\n    ],\n    \"totalGroups\": 0,\n    \"minImagesPerGroup\": 0,\n    \"maxImagesPerGroup\": 0\n  },\n  \"output\": {\n    \"info\": {}\n  },\n  \"imageGroups\": [\n    [\n      \"string\"\n    ]\n  ]\n}\n</code></pre> Key Description <code>id</code> Unique ID of the request. This value is generated automatically. <code>status</code> Status of the request: <code>completed</code>, <code>running</code>, or <code>failed</code> <code>callbackUrl</code> Displayed only if you have defined its value in the input parameters <code>input</code> As described in input Output info <code>inputImages</code> Number of input URLs provided <code>imagesConsidered</code> Number of valid URLs considered for grouping by the API <code>imageGroups</code> An array containing arrays of URLs. Each array represents a group of contextually similar images Exception <code>status</code> Status code of the error <code>title</code> Title of the error <code>detail</code> Additional information about the error"},{"location":"api-integration/api-air-1/#errors-and-exceptions","title":"Errors and Exceptions","text":"Status Title Detail 404 <code>invalidInput</code> Please provide the image URL as an input Request body is not a valid JSON Request parameter <code>url</code> is missing <code>badInput</code> Unable to load the image embedded in the URL <code>unreachableUrl</code> 404 Client Error: Not found for <code>url</code>:\"link of url\" <code>inputTooLarge</code> Image above 5 MB cannot be handled <code>inputTooSmall</code> Image size should be greater than 80 pixels"},{"location":"api-integration/api-air-1/#input-and-output-json-example","title":"Input and Output JSON Example","text":"<p>Input JSON</p> <p>In the following example, a list of input image URLs is provided to the Group-image API and the number of groups that should be created is specified as 2:</p> <pre><code>{\n  \"input\": {\n    \"urls\": [\n      \"https://upxxxxs.documents.cxxxs.io/vl/upxxxxs/f0842709-1adf-3fe9b4cc91bc~122?tenant=doc-platform\",\n      \"https://upxxxxs.documents.cxxxxxs.io/vl/upxxxxs/ae8ebdle-If79d038b82a~122 tenant=doc-platform\",\n      \"https://upxxxxs.documents.cxxxxxs.io/v1/upxxxxs/279e2406-544a570d6ee2~122?tenant=doc-platform\"\n    ],\n    \"totalGroups\": 2\n  }\n}\n</code></pre> <p>Output JSON</p> <p>The API segregates the batch of images into 2 groups, given as 2 arrays in the output: <pre><code>{\n  \"id\": \"242445f6-3eb2-4bd4-a61a-a610ef244de4\",\n  \"status\": \"Completed\",\n  \"output\": {\n    \"info\": {\n      \"inputImages\": 4,\n      \"imagesConsidered\": 4\n    },\n    \"imageGroups\": [\n      [\n        \"https://upxxxxs.documents.cxxxxxs.io/v1/upxxxxs/\u00a30842709-1adf-4475-b906-12036303281/spxx/a080bd1-5373-42c-8od2-?tentant=doc-platform\"\n      ],\n      [\n        \"https://upxxxxs.documents.cxxxxxs.io/v1/upxxxxs/279e2406-\u0435a54-434-807a-544a570d6ee2~122?tenant=doc-platform\",\n        \"https://upxxxxs.documents.cxxxxxs.io/v1/upxxxxs/279e2406-ea54-434-807a-15?tenant=doc-platform\",\n        \"https://upxxxxs.documents.cxxxxxs.io/v1/upxxxxs/214e2802-ea34-1234-807a-15?tenant=doc-platform\"\n      ],\n      \"input\",\n      \":\",\n      {\n        \"urls\": [\n          \"https://upxxxxs.documents.cxxxs.io/vl/upxxxxs/f0842709-1adf-3fe9b4cc91bc~122?tenant=doc-platform\",\n          \"https://upxxxxs.documents.cxxxxxs.io/vl/upxxxxs/ae8ebdle-If79d038b82a~122 tenant=doc-platform\",\n          \"https://upxxxxs.documents.cxxxxxs.io/v1/upxxxxs/279e2406-544a570d6ee2~122?tenant=doc-platform\"\n        ],\n        \"totalGroups\": 2\n      }\n    ]\n  }\n}\n</code></pre></p>"},{"location":"api-integration/api-am-1/","title":"Asset Management API","text":"<p>About this article </p> <p>Reference links to other articles are indicated but unavailable since these are sample documents.</p>"},{"location":"api-integration/api-am-1/#introduction","title":"Introduction","text":"<p>Albert is a micro-service available in Storage Services. Through its APls, Albert provides several digital asset management functionalities. For example, you can add custom metadata to your resource and run queries using that metadata. You can create relationships to link resources together and execute queries to find resources via those relationships.</p> <p></p>"},{"location":"api-integration/api-am-1/#resources","title":"Resources","text":""},{"location":"api-integration/api-am-1/#assets","title":"Assets","text":"<p>Asset refers to the resource(s) that the user will store.  </p> <p>Example 1   Resource URI and its metadata:</p> <pre><code>{\n     \"uri\": \"http://www.storage.cxxxxxs.io/objects/123\",\n     \"metadata\": {\n       \"objectSize\": 200,\n       \"BU\": \"vxxxpxxxnt\",\n       \"category\": \"face masks\"\n     }\n   }\n</code></pre> <p>Example 2  An image file and its metadata   &lt;file_icons.png&gt; along with its metadata snippet: <pre><code>{\n  \"metadata\": {\n    \"objectSize\": 200,\n    \"BU\": \"vxxxpxxxnt\",\n    \"category\": \"face masks\"\n  }\n}\n</code></pre></p>"},{"location":"api-integration/api-am-1/#types-of-assets","title":"Types of Assets","text":"<p>Managed Asset: The lifecycle and storage of the resource is controlled entirely by Albert. The user only needs to  provide the files or the URLs of the resource. Albert will generate a storage layer, and manage the creation, storage,  and deletion of the resource.</p> <p>Warning</p> <p>An asset can be either unmanaged or managed; never both.    At the time of creating an Asset in Storage Services, the option selected in the field <code>Managed</code>/ <code>Unmanaged</code> is permanent. It will remain unchanged throughout all versions of the Asset's lifecycle.</p> <p>Unmanaged Asset: The lifecycle of the resource is not controlled by Albert. It will merely store the URLs of the  resource that are provided by the user. Albert will not be involved in maintaining the resource.</p> <p>Info</p> <p>Refer to this article for information on Asset Operations.</p>"},{"location":"api-integration/api-am-1/#links","title":"Links","text":"<p>A link defines the relationship between two assets. The user can specify the type and nature of the relationship in any manner, and Albert helps to create and maintain the link. These links can then be used to query both assets.    </p> <p>Example : The user defines a relationship between the original image and an enhanced image of a resource. <code>Original Asset</code> &gt; <code>Enhance API</code> &gt; <code>Enhanced Asset</code>. With reference to the user-defined link <code>Enhance API</code>, the <code>Original Asset</code> is the source and the <code>Enhanced Asset</code> is the target asset.</p> <p>Info</p> <p>Refer to this article for information on Link Operations.</p>"},{"location":"api-integration/api-am-1/#features-why-use-albert","title":"Features - Why Use Albert?","text":"<p>As an Asset Management service, Albert enables users to: </p> <ul> <li> <p>Create Assets from their resources</p> </li> <li> <p>Add custom metadata to new and existing Assets</p> </li> <li> <p>Have a versioning system for each Asset</p> </li> <li> <p>Create user-defined relationships to link multiple Assets. A user can have any type of descriptive and queryable name for their links</p> </li> <li> <p>Query the Assets via metadata or links</p> </li> <li> <p>Control the lifecycle and storage multiple Assets in tandem with one another.</p> </li> </ul> <p>Example</p> <p>In this example, resources are referred to by their route for simplicity.</p> <p>A Business wants to link an original customer design to the enhanced version. Currently, these resources exist as two separate entities <code>/v1/upx/(uploadIdOfOriginal)</code> and <code>/v1/upx/(uploadIdOfEnhancedVersion)</code> respectively in the UPX API. These entities are created by different actors but are easily linked using Albert.    </p> <p>Now, if the Business wants to find the enhanced version of a customer's original upload, they can query <code>/v1/upx/{uploadIdOfOriginal}</code> and receive the contents of <code>/v1/upx/ {uploadIdOfEnhancedVersion}</code> in the result.</p>"},{"location":"api-integration/api-am-1/#future-use","title":"Future Use","text":"<ul> <li> <p>Aggregate the resources of a Business into Collections, which can be operated on as a group.</p> </li> <li> <p>Lifecycle and Auth management of linked assets</p> </li> <li> <p>Create multiple linked assets from a single input (with GWM integration)</p> </li> </ul> <p>Example </p> <p>When a customer uploads a design, the Original, Print, Preview, and Enhanced versions are generated and linked together.</p> <p>Support</p> <p>If you need any specific information or assistance, please get in touch with us:</p> <p>Slack: # <p>Email:"},{"location":"api-integration/api-am-2/","title":"Asset operations","text":"<p>About this article </p> <p>Reference links to other articles are indicated but unavailable since these are sample documents.</p> <p>Info</p> <p>To try out this API, go to Swagger.</p>"},{"location":"api-integration/api-am-2/#creating-an-asset","title":"Creating an Asset","text":"<p>You can create</p> <ul> <li> <p>a managed asset or </p> </li> <li> <p>an unmanaged asset</p> </li> </ul>"},{"location":"api-integration/api-am-2/#managed-asset","title":"Managed asset","text":""},{"location":"api-integration/api-am-2/#managed-asset-via-file","title":"Managed asset via file","text":"<p>Path: <code>v1/assets</code></p> <p>Request: <code>POST/v1/assets</code></p> <p>Content Type: multipart/ form-data</p> <p></p> <p>Description:</p> <ul> <li> <p>It must be a single level key-value pair only.</p> </li> <li> <p>It cannot have embedded objects, nested objects, or arrays.</p> </li> <li> <p>Length of key = Max. 128 characters (128 bytes) and value = 50* 1024 characters (50KB)</p> </li> </ul> <p><code>file</code>- file upload - (Required)</p> <ul> <li>Any valid binary file such as image, text, etc. Click Choose File to select the desired file.</li> </ul> <p><code>metadata</code>- object - (Optional)</p> <ul> <li>A key-value pair which provides metadata about the asset.</li> </ul> <p>Note</p> <ul> <li> <p>File size should not be greater than 8GB for production &amp; 4GB for non-production environment.    </p> </li> <li> <p>While creating managed assets, it is recommended to stream the metadata before the file.</p> </li> </ul> <p><code>expires</code>- string - (Optional, Default: <code>P30D</code>)</p> <ul> <li> <p>The date and time at which the asset will cease to exist. The value can either be a duration in ISO8601 format, or <code>never</code>. </p> </li> <li> <p>The value <code>never</code> indicates that the asset will exist perpetually i.e., without expiry. In asset and link responses, the same value is returned in the parameter <code>expiresOn</code>. If <code>\"expires\": \"never\"</code>, then <code>\"expiresOn\": null</code>.</p> </li> </ul>"},{"location":"api-integration/api-am-2/#managed-asset-via-uri","title":"Managed asset via URI","text":"<p>Path: <code>v1/assets</code></p> <p>Request: <code>POST/v1/assets</code></p> <p>Content Type: application/ json</p> <p>Input</p> <p><code>uri</code>- string - (Optional)</p> <ul> <li> <p>A valid http or https URL of the resource that you want to use as an asset.</p> </li> <li> <p>Length of <code>uri</code>= Max. 512 characters.</p> </li> <li> <p>When creating an asset via <code>uri</code>, you must additionally specify the <code>managed</code> parameter.</p> </li> </ul> <p><code>managed</code>- string - (Optional, Default: <code>true</code>)</p> <ul> <li> <p>Its value can be either <code>true</code> or <code>false</code>. This parameter value is permanent for a created asset and cannot be changed with any subsequent <code>PUT</code> or <code>PATCH</code> requests.</p> <ul> <li> <p><code>true</code> indicates a managed asset</p> </li> <li> <p><code>false</code> indicates an unmanaged asset.</p> </li> </ul> </li> </ul> <p><code>metadata</code>- object - (Optional)</p> <ul> <li> <p>A key-value pair which provides metadata about the asset.</p> </li> <li> <p>It must be a single level key-value pair only.</p> </li> <li> <p>It cannot have embedded objects, nested objects, or arrays.</p> </li> <li> <p>Length of key = Max. 128 characters (128 bytes) and value = 50* 1024 characters (50KB)</p> </li> </ul> <p>Note</p> <p>It is necessary to define at least one of the parameters: <code>uri</code> or <code>metadata</code> while creating an asset. - Size of file, obtained from the parameter <code>uri</code> should not be greater than 8GB for production and 4GB for non-production environment. - While creating managed assets, it is recommended to stream the metadata before the file.</p> <p><code>expires</code>: string - (Optional, Default: <code>P30D</code>)</p> <ul> <li> <p>The date and time at which the asset will cease to exist. The value can either be a duration in ISO8601 format, or <code>never</code>. </p> </li> <li> <p>The value <code>never</code> indicates that the asset will exist perpetually i.e., without expiry. In asset and link responses, the same value is returned in the parameter <code>expiresOn</code>. If <code>\"expires\": \"never\"</code>, then <code>\"expiresOn\": null</code>.</p> </li> </ul>"},{"location":"api-integration/api-am-2/#unmanaged-asset","title":"Unmanaged Asset","text":"<p>Path: <code>/v1/assets</code></p> <p>Request: <code>POST/v1/assets</code></p> <p>Content-Type: application/ json</p> <p>Input <pre><code>{\n  \"uri\": \"string\",\n  \"managed\": \"false\",\n  \"metadata\": \"object\",\n  \"expires\": \"string\"\n}\n</code></pre></p> <p><code>uri</code>- string - (Optional)</p> <ul> <li> <p>A valid http or https URL of the resource that you want to use as an asset.</p> </li> <li> <p>Length of 'uri'= Max. 512 characters</p> </li> <li> <p>When creating an asset via <code>uri</code>, you must additionally specify the <code>managed</code> parameter.</p> </li> </ul> <p><code>managed</code>- string - (Required with <code>uri</code> parameter)</p> <ul> <li> <p>To create an unmanaged asset via <code>uri</code>, it is mandatory to specify <code>managed: \"false\"</code>. </p> </li> <li> <p>This parameter value is permanent for a created asset and cannot be changed with any subsequent <code>PUT</code> or <code>PATCH</code> requests.</p> </li> </ul> <p><code>metadata</code> - object - (Optional)</p> <ul> <li> <p>A key-value pair which provides metadata about the asset.</p> </li> <li> <p>It must be a single level key-value pair only</p> </li> <li> <p>It cannot have embedded objects, nested objects, or arrays</p> </li> <li> <p>Length of key = Max. 128 characters (128 bytes) and value = 50* 1024 characters (50KB)</p> </li> </ul> <p>Note</p> <p>You must mandatorily define at least one of the parameters: <code>uri</code> or <code>metadata</code> while creating an asset.</p> <p><code>expires</code> - string - (Optional, Default: <code>P30D</code>)</p> <ul> <li> <p>The date and time at which the asset will cease to exist. The value can either be a duration in ISO8601 format, or <code>never</code>. </p> </li> <li> <p>The value <code>never</code> indicates that the asset will exist perpetually i.e., without expiry. In asset and link responses, the same value is returned in the parameter <code>expiresOn</code>. If <code>\"expires\": \"never\"</code>, then <code>\"expiresOn\": null</code>.</p> </li> </ul> <p></p>"},{"location":"api-integration/api-am-3/","title":"Updating an asset","text":"<p>About this article </p> <p>Reference links to other articles are indicated but unavailable since these are sample documents.</p> <p>Note</p> <ul> <li> <p>Currently, bulk update of assets is unsupported.</p> </li> <li> <p>You need to have a write access on the assetID in order to perform an update.</p> </li> </ul>"},{"location":"api-integration/api-am-3/#put-requests","title":"PUT Requests","text":"<p>Path: <code>/v1/assets/{assetID}</code></p> <p>Request: <code>PUT/v1/assets/{assetID}</code></p> <p>Content-Type: multipart/ form-data</p> <p>Headers: <code>If-Match</code> / <code>If-None-Match</code></p> <p>Input</p> <p><code>id</code>- string - (Required)</p> <ul> <li>ID of the asset which you want to update.</li> </ul> <p><code>If-Match</code> and <code>If-None-Match</code> - string - (Required)</p> <ul> <li>At least one of these headers are required while updating the asset, in order to maintain integrity and verify the correctness of the update. An Etag header will be received in the response after the asset is updated. For more details, click here.</li> </ul> <p>Request Body - (Required)</p> <p><code>file</code>: file upload - (Required)</p> <ul> <li>Any valid binary file such as image, text, etc. Provide either the actual file or the URL of the file.</li> </ul> <p><code>metadata</code> - object - (Optional)</p> <ul> <li>A key-value pair that you want to associate with the asset.</li> </ul> <pre><code>{\n  \"file\": \"string\",\n  \"metadata\": \"object\"\n}\n</code></pre>"},{"location":"api-integration/api-am-3/#patch-requests","title":"PATCH Requests","text":"<p>Path: <code>/v1/assets/{assetID}</code></p> <p>Request: <code>PATCH/v1/assets/{assetID}</code></p> <p>Content-Type: multipart/ form-data or application/ json</p> <p>Headers: <code>If-Match</code>/ <code>If-None-Match</code></p> <p>Input</p> <p><code>id</code>: string - (Required)</p> <ul> <li>ID of the asset which you want to update.</li> </ul> <p><code>If-Match</code> and <code>If-None-Match</code> - string - (Required)</p> <ul> <li>At least one of these headers are required while updating the asset, in order to maintain integrity and verify the correctness of the update. An Etag header will be received in the response after the asset is updated. For more details, click here.</li> </ul> <p>Request Body - (Required)</p> <p><code>file</code>- file upload - (Optional)</p> <ul> <li>Any valid binary file such as image, text, etc. Provide either the actual file or the URL of the file.</li> </ul> <p><code>metadata</code> - object - (Optional)</p> <ul> <li>A key-value pair that you want to merge with the existing metadata. Albert follows JSON patch standards as mentioned in this RFC.</li> </ul> <p><code>uri</code> - string - (Optional)</p> <ul> <li> <p>A valid http or https URL.</p> </li> <li> <p>This will create a new resource in the storage layer in the case of managed assets. </p> </li> <li> <p>Length of <code>uri</code> = Max. 512 characters</p> </li> </ul> <p><code>expires</code> - string - (Optional)</p> <ul> <li> <p>The date and time at which the asset will cease to exist. The value can either be a duration in ISO8601 format, or <code>never</code>. </p> </li> <li> <p>The value <code>never</code> indicates that the asset will exist perpetually i.e., without expiry. In asset and link responses, the same value is returned in the parameter <code>expiresOn</code>. If <code>\"expires\": \"never\"</code>, then <code>\"expiresOn\": null</code>. </p> </li> <li> <p>If the <code>expires</code> parameter has already been defined at the time of asset creation, the new <code>expiresOn</code> value = the existing <code>expires</code> value + the <code>expires</code> value defined in the <code>PATCH</code> request.</p> </li> </ul> <pre><code>{\n  \"file\": \"string\",\n  \"metadata\": \"object\",\n  \"uri\": \"string\",\n  \"expires\": \"string\"\n}\n</code></pre>"},{"location":"api-integration/api-am-4/","title":"Restoring an expired asset","text":"<p>About this article </p> <p>Reference links to other articles are indicated but unavailable since these are sample documents.</p> <p>A managed or unmanaged asset is eliminated once its <code>expiredOn</code> date passes. This, however, is a soft delete only. The user can pass the following query string parameter to view the expired asset, but will be unable to modify it in any way:</p> <p>Path: <code>/v1/assets/{id}</code> or <code>/v1/assets/route</code></p> <p>Request Type: <code>GET/v1/assets/{id}</code> or <code>GET/v1/assets/route</code></p> <p>Content-Type: application/ json</p> <p>Query Parameters: <code>showDeleted=true</code></p> <p>To fully restore an asset, use the following route:</p> <ul> <li> <p>Path: <code>/v1/assets/{id}:restore</code></p> </li> <li> <p>Request Type: <code>POST/v1/assets/{id}:restore</code></p> </li> <li> <p>Content-Type: application/ json</p> </li> </ul> <p><pre><code>{\n\"expires\": \"string\"\n}\n</code></pre> <code>expires</code> - string - (Optional)</p> <ul> <li> <p>When a value is defined, it will be used to calculate the expiration. Otherwise, the expiration of the restored asset will be calculated from the original <code>expires</code> field. The value can either be a duration in ISO8601 format, or <code>never</code>. </p> </li> <li> <p>The value <code>never</code> indicates that the asset will exist perpetually i.e., without expiry. In asset and link responses, the same value is returned in the parameter <code>expiresOn</code> If <code>\"expires\": \"never\"</code>, then <code>\"expiresOn\": null</code>.</p> </li> </ul> <p>Note</p> <p>By updating the <code>expires</code> field, you can use this route to extend the expiration of a non-expired asset as an alternative method to using <code>PATCH</code> requests described in the earlier article.</p>"},{"location":"api-integration/api-am-5/","title":"Deleting an asset","text":"<p>About this article </p> <p>Reference links to other articles are indicated but unavailable since these are sample documents.</p> <p>This will eliminate the entire asset i.e., the current as well as previous versions. For managed assets, it also includes deleting the corresponding file resources. All versions will be marked as <code>deleted</code> and will be permanently deleted after a period of 90 days.</p> <p>Path: <code>/v1/assets/{assetID}</code></p> <p>Request: <code>DELETE/v1/assets/{assetID}</code></p> <p>Headers:<code>If-Match</code> / <code>If-None-Match</code></p> <p>Input</p> <p><code>id</code> - string - (Required) </p> <ul> <li>ID of the asset which you want to delete.</li> </ul> <p><code>If-Match</code> and <code>If-None-Match</code> - string - (Required) </p> <ul> <li>At least one of these headers are required while deleting the asset, in order to maintain integrity and verify the correctness of the deletion. An Etag header will be received in the response after the asset is updated. For more details, click here.</li> </ul> <p><code>purge</code> - boolean - (Optional, Default: <code>false</code>)</p> <ul> <li> <p>This is a query parameter, and the value is either <code>true</code> or <code>false</code>.    </p> </li> <li> <p>When <code>\"purge\": true</code>, it will permanently eliminate, with immediate effect, the entire asset i.e., the current as well as previous versions. For managed assets, it also includes deleting the corresponding file resources.</p> </li> <li> <p>After Purging, the asset cannot be recovered since this is an immediate and permanent deletion. When <code>\"purge\": false</code>, all versions of the asset will be marked as deleted and will be permanently deleted after a period of 90 days.</p> </li> </ul> <p></p> <p></p> <p>Note</p> <ul> <li>Currently, bulk deletion of assets is unsupported.    </li> <li>You need to have the requisite permissions in order to perform a deletion.</li> </ul>"},{"location":"api-integration/api-am-6/","title":"Reading and querying an asset","text":""},{"location":"api-integration/api-am-6/#reading-an-asset","title":"Reading an Asset","text":"<p>This allows you to retrieve information within an asset. You can read an asset in the following ways:</p> Description Request Path To retrieve all the assets <code>GET/v1/assets</code> To retrieve a specific asset <code>GET/v1/assets/{assetID}</code> To retrieve all the links associated with a specific asset as the source <code>GET/v1/assets/{assetID}/links</code> To retrieve all versions of a specific asset <code>GET/v1/assets/{assetID}/versions</code> To retrieve a specific version of an asset <code>GET/v1/assets/{assetID}/versions/ {versionID}</code>"},{"location":"api-integration/api-am-6/#querying-an-asset","title":"Querying an Asset","text":"<p>Assets can be searched based on:</p> <ul> <li> <p>the metadata present in them</p> </li> <li> <p>the URI of the resources within the asset</p> </li> </ul> <p>Path: <code>/v1/assets/: query</code></p> <p>Request Type: <code>POST/v1/assets/: query</code></p> <p>Content-Type: application/ json</p> <p>Query Parameters <code>limit</code> ,<code>offset</code>, <code>sort</code>, <code>showDeleted</code></p> <p>Input</p> <p><code>uri</code>- string - (Optional)</p> <ul> <li> <p>A valid http or hips URL of the asset that you want to query. </p> </li> <li> <p>Length of <code>uri</code>= Max. 512 characters</p> </li> </ul> <p><code>metadata</code> - object - (Optional)</p> <ul> <li>A key-value pair associated with the asset.</li> </ul> <p><code>managed</code>- boolean - (Optional. Default: <code>false</code>)</p> <ul> <li> <p>This is a query parameter, and the value is either <code>true</code> or <code>false</code>. </p> </li> <li> <p>This parameter filters the search based on managed or unmanaged assets respectively.</p> </li> </ul> <p><pre><code>{\n  \"uri\": \"string\",\n  \"metadata\": \"object\",\n  \"managed\": boolean\n}\n</code></pre> Example JSON: Managed Assets</p> <p>This shall query all the managed assets that contain <code>\"team\": \"mumbai\"</code> as one of the key- value pairs in their metadata. </p> <pre><code>{\n  \"metadata\": {\n    \"team\": \"mumbai\"\n  },\n  \"managed\": true\n}\n</code></pre> <p>Example JSON: Unmanaged Assets</p> <p>This shall query all the unmanaged assets that contain <code>\"team\" : \"mumbai\"</code> as one of the key-value pairs in their metadata.</p> <pre><code>{\n  \"metadata\": {\n    \"team\": \"mumbai\"\n  },\n  \"managed\": false\n}\n</code></pre> <p>Additionally, the following query parameters can be used. When multiple query parameters are defined, an <code>AND</code> operation will be performed to combine them.</p> <p><code>limit</code>: number - (Optional. Default: <code>10000</code>)</p> <ul> <li>Number of records to return in a single page. For example, fetch <code>100</code> records that meet the condition.</li> </ul> <p><code>offset</code>- string - (Optional. Default: <code>beginning of page</code>)</p> <ul> <li>Page offset to continue the search and retrieve the next page.</li> </ul> <p><code>sort</code>- string - (Optional. Default: <code>createdMS:desc</code>)</p> <ul> <li>Defines the direction of the sorting. This field can have one of the following values: <code>createdMs: asc</code>, <code>modifiedMs: desc</code>, and <code>modifiedMs: asc</code> and <code>modifiedMs: desc</code> </li> </ul> <p><code>showDeleted</code>- boolean - (Optional. Default: <code>false</code>)</p> <ul> <li>When <code>true</code>, deleted assets will also be included in the query results.</li> </ul> <p>Support</p> <p>If you've hit a roadblock, reach out to us through any of the following support channels:    </p> <p>Email: helpid@cxxxxxs.com</p> <p>Slack: #"},{"location":"api-integration/data-export/","title":"Snowflake instance to SW3","text":"<p>About this article </p> <p>Reference links to other articles are indicated but unavailable since these are sample documents.</p> <p>This article describes items that need to be set up by the customer before proceeding with other Snowflake- AWS S3 actions in Zumo AI.</p>"},{"location":"api-integration/data-export/#step-1-creating-an-iam-policy","title":"Step 1: Creating an IAM policy","text":"<ul> <li>Log into the AWS Management Console.</li> <li>In the Home dashboard, search for and select IAM.</li> <li>From the left-hand navigation pane, select Account settings.</li> <li>Under Security Token Service (STS), in the Endpoints list, find the Snowflake region - US West (N. California). If the STS status is inactive, slide the toggle to Active.</li> <li>From the left-hand navigation pane, click Policies.</li> <li>Select Create Policy.</li> <li>For Policy editor, select JSON.</li> <li>Add a policy document that will allow Snowflake to access the S3 bucket and folder.  </li> </ul> <p>The following policy (in JSON format) provides Snowflake with the required permissions to load data using a single bucket and folder path.</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"VisualEditor0\",\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:ListBucket\",\n      \"Resource\": \"arn:aws:s3:::&lt;your-bucket-name&gt;\"\n    },\n    {\n      \"Sid\": \"VisualEditor1\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:GetObject\"\n      ],\n      \"Resource\": \"arn:aws:s3:::&lt;your-bucket-name&gt;/*\"\n    }\n  ]\n}\n</code></pre> <ul> <li>Click Next.</li> <li>Enter a Policy name, for example, snowflake_access and optionally, a Description.</li> <li>Click Create policy.</li> </ul>"},{"location":"api-integration/data-export/#step-2-create-the-iam-role-in-aws","title":"Step 2: Create the IAM Role in AWS","text":"<p>To configure access permissions for Snowflake in the AWS Management Console, do the following:</p> <ul> <li>From the left-hand navigation pane in the Identity and Access Management (IAM) dashboard, click Roles.</li> <li>Select Create role.</li> <li>Select AWS account as the trusted entity type.</li> <li>In the Account ID field, enter your own AWS account ID temporarily. In later steps, you will modify the trust relationship and grant access to Snowflake.</li> <li> <p>Select the Require external ID option. An external ID is used to grant access to your AWS resources (such as S3 buckets) to a third party such as Snowflake.    Enter a placeholder ID such as 0000. In later steps, you will modify the trust relationship for your IAM role and specify the external ID for your storage integration. Click Next. </p> </li> <li> <p>Select the policy you created in Step 1.     Click Next.</p> </li> <li>Enter a name and description for the role, then click Create role.</li> </ul> <p>You have now created an IAM policy for a bucket, created an IAM role, and attached the policy to the role.</p> <p>On the Role summary page, locate and record the Role ARN value.</p> <p>Note</p> <p>Once the above steps are complete, share the Role ARN value and bucket name with Zumo AI.</p> <p>The support team will get back to you with two more details: STORAGE_AWS_IAM_USER_ARN and STORAGE_AWS_EXTERNAL_ID. </p> <p>Once you have received these values, proceed to complete the actions described in Step 3 below.</p>"},{"location":"api-integration/data-export/#step-3-allowing-the-zumo-iam-user-to-assume-the-role","title":"Step 3: Allowing the Zumo IAM User to assume the role","text":"<p>The following step-by-step instructions describe how to configure IAM access permissions for Snowflake in your AWS Management Console.</p> <ul> <li>Log in to the AWS Management Console and click IAM.</li> <li>From the left-hand navigation pane, click Roles.</li> <li>Select the role you created in Step 2.</li> <li>Open the Trust relationships tab.</li> <li>Click Edit trust policy.</li> <li>Modify the policy document with the 2 values given to you by Zumo AI i.e., STORAGE_AWS_IAM_USER_ARN and STORAGE_AWS_EXTERNAL_ID where,<ul> <li>snowflake_user_arn is the STORAGE_AWS_IAM_USER_ARN value Zumo AI shared with you.</li> <li>snowflake_external_id is the STORAGE_AWS_EXTERNAL_ID value Zumo AI shared with you.</li> </ul> </li> </ul> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"&lt;snowflake_user_arn&gt;\"\n      },\n      \"Action\": \"sts:AssumeRole\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"sts:ExternalId\": \"&lt;snowflake_external_id&gt;\"\n        }\n      }\n    }\n  ]\n}\n</code></pre> <p>Upon receiving your confirmation, we will proceed with configuring the exports on our end. If you have any questions or encounter any issues, please reach out to your\u00a0Zumo AI\u00a0Account Executive/ Point of Contact.</p>"},{"location":"api-integration/integration/","title":"Nice inContact <> Instance Integration","text":"<p>About this article </p> <p>Reference links to other articles are indicated but unavailable since these are sample documents.</p> <p>As an Admin or Super Admin, you can independently configure the Nice inContact integration within your Acme AI instance. This is a self-service option and eliminates the need to contact your Technical Account Managers (TAMs) for initial setup.</p> <p>NOTE</p> <p>To create a user account for use with the Acme AI integration along with the required Access Key ID and Access Key Secret, open the NICE CXone Mpower Getting Started page and follow the steps listed in the section Task 3: Prepare a Test User Account.</p>"},{"location":"api-integration/integration/#a-initiating-the-nice-incontact-integration","title":"(A) Initiating the Nice inContact Integration","text":"<ol> <li>In the left navigation bar, click Settings &gt; click Integrations.</li> <li> <p>Locate the Nice inContact integration option among the available integrations and click it.  </p> <p></p> </li> </ol>"},{"location":"api-integration/integration/#b-connecting-nice-incontact-to-your-acme-ai-instance","title":"(B) Connecting Nice inContact to your Acme AI Instance","text":"<ol> <li> <p>You will be prompted to provide the necessary connection credentials. These credentials authorize Acme AI to securely access and retrieve data from your Nice inContact environment. Please ensure you have the following information readily available and enter them in the corresponding fields:</p> </li> <li> <p>Access Key ID: A unique identifier for the Acme AI application within your Nice inContact instance.</p> </li> <li> <p>Access Key Secret: A confidential key associated with the Access Key ID, used for secure authentication.  </p> <p></p> </li> <li> <p>You will be redirected back to the Acme AI platform and your Nice inContact account will be displayed under Enabled Accounts, indicating a successful authentication.  </p> <p></p> </li> </ol>"},{"location":"api-integration/integration/#c-selecting-and-configuring-import-channels","title":"(C) Selecting and Configuring Import Channels","text":"<ol> <li> <p>Under Enabled Accounts, click the  button &gt; Configure.  </p> <p></p> </li> </ol>"},{"location":"api-integration/integration/#d-configuring-data-ingestion-parameters","title":"(D) Configuring Data Ingestion Parameters","text":"<p>Acme AI provides several parameters to filter and refine the data ingested from Nice inContact. These configurations allow you to focus on the most relevant data for your analysis. The following configuration options are available:</p> <p>(i) Language Translation: You can configure Language translation by turning on the flag and entering the relevant metadata. You can select the field name, operator (Equals, Starts with, Ends with) and field value to identify the relevant language of the ticket.</p> <p>(ii) Data Filtering: Refine imports by Team name, Campaign Name, and Skill Name</p> <p>(iii) Minimum Agent Handling Time: Configure the minimum agent handling time to be ingested into Acme AI. Calls having agent handling time greater than this value would be ingested.</p> <p>(iv) Minimum Contact Duration: Configure the minimum total call time to be ingested into Acme AI. Calls having total time greater than this value would be ingested.</p> <p>(v) Include Outbound calls and Include IVR Calls: Slide the respective toggle to enable or disable ingestion of outbound and IVR calls into your Acme instance.  </p> <p></p> <p>Redaction and Booster Words: You can enable and manage the Redaction and Booster words features for your imported data. This allows for sensitive data masking and highlighting of critical terms, enabling better compliance and transcription accuracy.</p> <p>After configuring your preferences, click Next to proceed.</p>"},{"location":"api-integration/integration/#e-importing-sample-data-for-verification","title":"(E) Importing Sample Data for Verification","text":"<p>Before fully enabling the integration, it is highly recommended to import a sample of your Nice inContact data into Acme AI. This allows you to verify your configured filters and ensure the data is being ingested as expected.  </p> <p></p> <p>You can choose to import data from the last 6, 12, or 24 hours.  </p> <p>NOTE</p> <ul> <li>The sample conversation import is limited to a maximum of 1000 conversations, regardless of the time period selected.  </li> <li>Allow approximately 15-30 minutes for the sample conversations to load into Acme AI.</li> </ul>"},{"location":"api-integration/integration/#f-reviewing-and-enabling-the-integration","title":"(F) Reviewing and Enabling the Integration","text":"<ol> <li>Once the sample data has been imported, carefully review each item in the provided checklist to confirm the accuracy of your configurations and the flow of conversations. Click the corresponding  icon for for detailed instructions and screenshots.</li> <li> <p>After you have thoroughly verified the sample data and are confident in your configurations, mark all the checkboxes in the checklist and click Enable to activate your Nice inContact integration.  </p> <p></p> </li> <li> <p>In the dialog box that appears, click Enable to finalize your integration.  </p> <p></p> </li> </ol>"},{"location":"api-integration/integration/#g-editing-integration-configurations","title":"(G) Editing Integration Configurations","text":""},{"location":"api-integration/integration/#pausing-the-data-pipeline","title":"Pausing the Data Pipeline","text":"<p>You can temporarily halt data ingestion from Nice inContact to Acme AI at any time.</p> <ol> <li> <p>Under Enabled Accounts, click the  button &gt; click Pause.  </p> <p></p> </li> <li> <p>Confirm the action in the dialog box that appears.</p> </li> <li>The account status under Connected Accounts will update to indicate that the pipeline is paused.</li> </ol>"},{"location":"api-integration/integration/#disconnecting-the-account","title":"Disconnecting the Account","text":"<p>If you no longer wish to continue with a specific Nice inContact account for importing data into Acme AI, you can disconnect it.</p> <ol> <li> <p>Under Enabled Accounts, click the  button &gt; click Disconnect.  </p> <p> </p> </li> <li> <p>Confirm the action in the dialog box that appears.     The pipeline will be disconnected, and the account will be removed from Enabled Accounts and Connected Accounts.</p> </li> </ol>"},{"location":"api-integration/integration/#renaming-the-account","title":"Renaming the Account","text":"<p>You can easily rename your connected Nice inContact account for better organization.</p> <ol> <li>Under Enabled Accounts, click the  button &gt; click Rename.</li> <li> <p>Enter the desired new name for the account in the provided field. The account will now display its new name.  </p> <p></p> </li> <li> <p>Enter the desired new name for the account in the provided field. The account will now display its new name.  </p> <p></p> </li> </ol>"},{"location":"api-integration/integration/#connecting-new-accounts","title":"Connecting New Accounts","text":"<p>You can integrate multiple Nice inContact accounts with your Acme AI instance:</p> <ol> <li> <p>In the Nice inContact Integration page, click Add Account.  </p> <p> </p> </li> <li> <p>In the popup that appears, enter the credentials of the new Nice inContact account you wish to connect, then click Connect to Nice inContact.</p> </li> <li>Continue by following the authentication steps outlined here.</li> </ol>"},{"location":"api-integration/integration/#frequently-asked-questions-faqs","title":"Frequently Asked Questions (FAQs)","text":"<p>Q. What are the most frequently used configuration options in the Nice inContact self-serve integration? A. Users commonly configure the integration to: - Filter tickets by Team Name, Campaign Name and Skill Name - Configure Call duration, Minimum Agent Handling Time, Minimum Contact Duration - Configure call filters such as Include Outbound Calls and Include IVR Calls - Enable Redaction and Booster words for better analysis - Language Configuration</p> <p>Q. How often is data imported from Nice inContact to Acme AI? A. The integration supports data import once every hour. This ensures near real-time visibility of your data without overloading the system.</p> <p>Q. Can historical Nice inContact calls data be pulled through the self-serve interface? A. No, historical data pull is not supported in the self-serve setup. For historical imports, please contact your Technical Account Manager or the Acme AI support team for assistance.</p>"},{"location":"blogs/","title":"Tech Blogs","text":"<p>Speech to Text \u2192</p> <p>How Speech-to-Text Transcription works \u2192</p> <p></p>"},{"location":"blogs/speech-to-text-1/","title":"Speech to Text","text":"<p>In today's fast-paced world, understanding every customer interaction is crucial. Nowhere is this more critical than in contact centers, where every word matters. Enter Speech-to-Text (STT): the unsung hero of customer experience. Let\u2019s explore how this powerful technology, and STT solutions such as Zumo AI's Orba, are transforming the way businesses listen, learn, and lead.</p>"},{"location":"blogs/speech-to-text-1/#what-is-speech-to-text-stt","title":"What is Speech-to-Text (STT)?","text":"<p>Speech-to-Text (also known as Automatic Speech Recognition or ASR) converts spoken language into text. It\u2019s like a superpowered digital notetaker; instantly transcribing customer calls, sales conversations, or supporting chats into searchable text. From everyday voice assistants such as Siri and Alexa to high-stakes use in healthcare or legal settings, STT is everywhere! </p> <p></p>"},{"location":"blogs/speech-to-text-1/#when-stt-fails-so-does-cx","title":"When STT Fails, So Does CX","text":"<p>The accuracy of STT is non-negotiable. Imagine an agent trying to resolve an issue based on a garbled transcript. Poor STT quality can lead to:</p> <ul> <li>Misunderstandings: Incorrect transcriptions can lead to errors in customer service and data analysis. For example, a customer saying \"product ID X-250B\" could be transcribed as \"eggs two fifty bee,\" causing confusion all around.  </li> <li>Lost Insights: Missed keywords and sentiment can hide crucial customer pain points; businesses lose valuable data that could be the foundation for major product improvements or service enhancements. For instance, you might miss a surge of calls complaining about a particular service if the STT isn't accurate enough to flag it.  </li> <li>Wasted Time: Correcting errors and clarifying misunderstandings takes time and resources, adding to operational costs. Agents have to manually verify information that the STT system got wrong, slowing down the entire chain of service.</li> </ul> <p>Bad STT means higher costs, frustrated customers, and missed opportunities. So, how do you avoid this trap?</p> <p>Test! Test! Test! You must always cross-check whether an STT solution is compatible with your data.</p>"},{"location":"blogs/speech-to-text-1/#not-all-stt-are-created-equal","title":"Not All STT Are Created Equal","text":"<p>STT performance is often measured by Word Error Rate (WER). However, it has been observed frequently that any STT solution built over generic AI models struggle to remain accurate in real-world contact center scenarios\u2014 they\u2019re unable to recognize industry-specific jargon, complex alphanumeric strings such as account numbers or product codes and moreover, unable to deal with accents, dialects and background noise. This is where customization is crucial. An STT solution that allows you to finetune the underlying AI models with your unique industry data significantly improves accuracy, making them truly effective for your business.</p> <p></p> <p>Does such a solution actually exist? Yes, indeed!</p>"},{"location":"blogs/speech-to-text-1/#meet-orba-the-only-stt-built-for-contact-centers","title":"Meet Orba: The only STT Built for Contact Centers!","text":"<p>Zumo AI\u2019s Orba isn\u2019t just another STT engine. It\u2019s purpose-built for contact centers, trained on vast amounts of real support conversations across industries such as finance, healthcare, and retail. Orba doesn\u2019t just hear\u2014it understands.</p> <p>Under the Hood</p> <p>Unlike other solutions built on rented LLMs and generic APIs, Orba is powered entirely by Zumo AI\u2019s proprietary models\u2014built and hosted on our own GPUs, tuned for the unique demands of customer experience workflows.</p> <p>This means:</p> <ul> <li>Better control over accuracy  </li> <li>Faster performance (just 20 seconds per audio hour)  </li> <li>Lower cost ($0.0043/minute vs. $0.024/minute for others)  </li> <li>Full customization with context biasing, keyword boosting, and diarization for call transfers</li> </ul>"},{"location":"blogs/speech-to-text-1/#why-orba-stands-out","title":"Why Orba Stands Out","text":"<p>\u2714\ufe0f Industry Smarts: Trained on contact center-specific language and use cases. \u2714\ufe0f Pinpoint Accuracy: Gets product names, account numbers, and jargon right minimizing costly errors. \u2714\ufe0f Deep Customization: Adapts to your vocabulary, brand, and even agent accents. \u2714\ufe0fScalable: Handles millions of conversations for numerous enterprise customers without breaking a sweat \u2714\ufe0f Unrivaled Efficiency: Faster transcription and data processing improve operational efficiency across the board. \u2714\ufe0f Valuable Insights: Highly accurate data fuels robust analytics, powers intelligent quality assurance, and enables smarter strategic decisions in your organization.</p> <p>How Orba Stacks Up</p> <p>Compared to other transcription providers, Orba offers unmatched value:</p> <p></p> <p>WER \\= Word Error Rate; lower is better. Orba is optimized for noisy, real-world customer service calls.</p>"},{"location":"blogs/speech-to-text-1/#the-roi-is-real","title":"The ROI Is Real","text":"<p>Companies using Orba see results! </p> <p></p> <ul> <li>Viz: Saved $30 million by reducing refunds through better call insights.  </li> <li>Crater: Cut average handle time by 20%, improving agent efficiency.  </li> <li>Akkctt: Reduced call volumes by 35% with STT-powered self-service strategies.</li> </ul>"},{"location":"blogs/speech-to-text-1/#summing-it-all-up-stt-isnt-just-techits-strategy","title":"Summing it all up: STT Isn\u2019t Just Tech\u2014It\u2019s Strategy","text":"<p>STT is no longer optional, it\u2019s essential for contact centers aiming to scale smartly, serve better, and compete harder. But not just any STT will do. With Zumo AI\u2019s Orba, you're not just transcribing conversations\u2014you\u2019re turning them into actionable intelligence.</p> <p>Want to improve CX, reduce costs, and drive real results? Start by listening better with Orba.</p>"},{"location":"blogs/speech-to-text-2/","title":"How Speech-to-Text Transcription Works","text":"<p>Ever wonder how your voice assistant understands you, or how customer calls get transcribed into neat, searchable text? It all starts with Automatic Speech Recognition (ASR). ASR is the technology that turns spoken words into written text, acting as the silent workhorse behind everything from transcribing a customer support call, powering voice assistants, to generating real-time captions.</p> <p>This blog unpacks the core components of ASR technology. We'll explore how each stage contributes to accurate, context-aware transcription, and specifically how Zumo AI's proprietary model, Orba, is engineered for the unique demands of contact center conversations.</p>"},{"location":"blogs/speech-to-text-2/#the-five-pillars-of-asr-systems","title":"The Five Pillars of ASR Systems","text":"<p>Accurate speech-to-text conversion is the result of a multi-stage process\u2014each component plays a critical role in ensuring the output is not just technically correct, but useful and human-readable. Here's a closer look:</p> <p></p>"},{"location":"blogs/speech-to-text-2/#1-voice-activity-detection-vad","title":"1. Voice Activity Detection (VAD)","text":"<p>VAD is the ASR system\u2019s gatekeeper. Its job is to filter the audio stream and detect when actual speech occurs;   separating it from silence, background music, coughs, or keyboard clicks.</p> <p>How it works: VAD uses short-time energy and spectral features of the audio signal to flag segments that contain voice activity. In modern systems, deep learning models like CNNs (Convolutional Neural Networks) or RNNs (Recurrent Neural Networks) trained on diverse audio environments provide more robust detection, even in noisy settings.</p> <p>Real-world Example: During a call, an agent might put a customer on hold. VAD identifies the call hold music as non-speech and ensures it is not passed to later ASR stages, preserving compute resources and reducing false positives in transcription.</p> <p>Why it matters: Without VAD, background noise and silence would clutter the system with irrelevant data; leading to erroneous transcripts, increased latency, and unnecessary compute usage.</p>"},{"location":"blogs/speech-to-text-2/#2-acoustic-modeling-am","title":"2. Acoustic Modeling (AM)","text":"<p>Once VAD isolates the speech regions, the AM takes over. It analyzes short fragments of the audio; typically 20 to 30 milliseconds, and converts them into phonetic or character-level probabilities.</p> <p>How it works: The AM maps audio features e.g., MFCCs (Mel-Frequency Cepstral Coefficients), spectrograms etc. to linguistic units using models like HMMs (Hidden Markov Models), DNNs (Deep Neural Networks), or end-to-end transformers (like Conformers). It is trained on thousands of hours of labeled speech data to learn how sound patterns correspond to sounds in language.</p> <p>Real-world Example: When a customer says \u201ctracking number\u201d the AM hears the raw sound waves and produces something like:  t-r-a-k-i-ng n-u-m-b-e-r.</p> <p>Challenges:</p> <ul> <li> <p>Homophones: The AM alone cannot distinguish between \u201cthere\u201d and \u201ctheir\u201d because they sound identical.</p> </li> <li> <p>Accents and emotions: It must account for various accents, speaking speeds, and expressions of emotion (anger, confusion), which can distort pronunciation.</p> </li> </ul>"},{"location":"blogs/speech-to-text-2/#3-language-modeling-lm","title":"3. Language Modeling (LM)","text":"<p>This is where context comes into play. The LM takes the noisy or uncertain outputs from the acoustic model and uses contextual knowledge to form coherent, grammatically and semantically meaningful phrases.</p> <p>How it works: LMs are trained on massive amounts of text data to understand how words typically appear in sequences.  They learn probabilities of word sequences; e.g., \u201ccancel my order\u201d is far more likely than \u201ccandle my order\u201d, even if both sound similar. Advanced LMs use n-grams, RNNs, or Transformers (like GPT-style models) to accurately predict word sequences.</p> <p>Real-world Example: If the AM outputs \u201cI want to no my bill\u201d the LM corrects it to: \u201cI want to know my bill\u201d  because it understands that \u201cknow\u201d is more probable in this context than \u201cno\u201d.</p> <p>Why it matters: LMs are crucial for resolving ambiguity, distinguishing homophones, and correcting mispronunciations; making transcripts readable, accurate, and aligned with how humans interpret language.</p>"},{"location":"blogs/speech-to-text-2/#4-speaker-diarization-sd","title":"4. Speaker Diarization (SD)","text":"<p>In real-world conversations, especially contact center calls, both participants speak on the same channel. SD helps determine who spoke when.</p> <p>How it works: The model segments the audio and clusters parts of the waveform, assigning them to different speakers.This involves  using embeddings (like x-vectors) and clustering algorithms (e.g., spectral clustering) to identify distinct voices. Some systems also use auxiliary data like speaker IDs in multi-channel recordings.</p> <p>Real-world Example: In a call like:</p> <ul> <li> <p>Speaker A: \u201cI\u2019m having issues with my internet.\u201d</p> </li> <li> <p>Speaker B: \u201cLet me check that for you.\u201d</p> </li> </ul> <p>SD ensures that the transcript is structured like:  Customer: I\u2019m having issues with my internet.   Agent: Let me check that for you.</p> <p>Why it matters: Without SD, the transcript would be a confusing block of text, making it impossible to separate agent responses from customer concerns; undermining both analytics and usability.</p>"},{"location":"blogs/speech-to-text-2/#5-inverse-text-normalization-itn","title":"5. Inverse Text Normalization (ITN)","text":"<p>Raw ASR output often contains literal transcriptions of how things sound, not how they should appear in writing. ITN converts spoken phrases into their proper standardized, readable formats.</p> <p>How it works: ITN applies rule-based or ML-based mappings to detect and normalize numbers, dates, times, currencies, abbreviations, and common entities.</p> <p>Real-world Example:  Spoken: \u201cThe total is seventy eight dollars and fifty five cents\u201d  Raw ASR: \u201cseventy eight dollars and fifty five cents\u201d  ITN Output (Transcribed): $78.55</p> <p>Spoken: \u201ccall me at eight zero eight three two one four seven nine nine\u201d  Output: Call me at 808-321-4799</p> <p>Why it matters: ITN makes the transcript actionable, especially for downstream systems such as analytics engines, chatbots, or CRM integrations.</p> <p>Speech-to-text transcription is not just about recognizing words; it is about understanding who is speaking, what is being said, and how it should be interpreted. From detecting voice activity in noisy environments to resolving homophones with language context, each component plays a vital role.</p>"},{"location":"blogs/speech-to-text-2/#powered-by-conversations-zumo-ais-in-house-asr-model-orba","title":"Powered by Conversations: Zumo AI's In-House ASR Model - Orba","text":"<p>At Zumo AI, our transcription engine, Orba, isn\u2019t just another generic speech-to-text tool, it is a domain-trained ASR system built for contact centers. Our in-house model is trained on millions of hours of conversational data spanning industries like telecom, retail, healthcare, and finance.</p> <p></p> <p>This deep, real-world exposure enables Orba to:</p> <ul> <li> <p>Understand diverse accents, interruptions, and informal speech common in support calls.</p> </li> <li> <p>Handle domain-specific terms with precision, such as \u201cclaim number\u201d, \u201cplan ID\u201d, or \u201crouting issue\u201d.</p> </li> <li> <p>Adapt to the dynamic conversational flow of agent-customer interactions, where intent and context shift rapidly.</p> </li> </ul> <p>The result is a high-accuracy, real-time ASR system that truly understands the language of support.</p>"},{"location":"how-tos-faqs/","title":"How-to articles and Frequently Asked Questions (FAQs) samples","text":""},{"location":"how-tos-faqs/#how-tos","title":"How-tos","text":"<p>How to find conversations in the platform \u2192 </p>"},{"location":"how-tos-faqs/#faqs","title":"FAQs","text":"<p>FAQs for Agents and Managers \u2192</p> <p></p>"},{"location":"how-tos-faqs/faq-agents-managers/","title":"FAQs for Agents and Managers","text":""},{"location":"how-tos-faqs/faq-agents-managers/#for-agents","title":"For Agents","text":""},{"location":"how-tos-faqs/faq-agents-managers/#how-do-i-log-in","title":"How do I log in?","text":"<p>A: You can either use the SSOs available with your organization or use your Zumo AI credentials (please check with your IT department if you don\u2019t have this).</p>"},{"location":"how-tos-faqs/faq-agents-managers/#when-do-knowledge-cards-pop-up","title":"When do Knowledge Cards pop up?","text":"<p>A: Knowledge cards automatically appear when the AI system detects something in the conversation that can be explained with an article that is currently available in your company's Knowledge Base.</p>"},{"location":"how-tos-faqs/faq-agents-managers/#why-should-i-trust-agentgpt-responses","title":"Why should I trust AgentGPT responses?","text":"<p>A: The underlying sources from which AgentGPT extracts information is provided right below the response, so you can verify its accuracy.</p>"},{"location":"how-tos-faqs/faq-agents-managers/#how-do-i-give-feedback-when-the-ai-is-wrong","title":"How do I give feedback when the AI is wrong?","text":"<p>A: Please use the thumbs up and thumbs down buttons to provide feedback to the AI system, which should improve over time. </p>"},{"location":"how-tos-faqs/faq-agents-managers/#what-do-i-do-if-the-conversation-is-not-loading-in-agent-assist","title":"What do I do if the conversation is not loading in Agent Assist?","text":"<p>A: If nothing is loading at all, including the real-time transcript at the bottom, it most likely is an issue with streaming the calls from your CCaaS. Please reach out to your manager to raise a ticket and the Zumo AI Support team will follow up accordingly.  </p>"},{"location":"how-tos-faqs/faq-agents-managers/#for-managers","title":"For Managers","text":""},{"location":"how-tos-faqs/faq-agents-managers/#what-do-i-do-if-conversations-are-not-loading-in-manager-assist","title":"What do I do if conversations are not loading in Manager Assist?","text":"<p>A: If conversations are not matching with your CCaaS or other system of record, please raise a ticket and the Zumo AI Support team will follow up accordingly.</p>"},{"location":"how-tos-faqs/faq-agents-managers/#how-do-i-add-a-live-conversation-to-a-future-coaching-session","title":"How do I add a live conversation to a future coaching session?","text":"<p>A: Hover your cursor over the row containing the desired conversation. The Add to Coaching icon  appears; click  to add the conversation to either a new or existing coaching session. </p> <p></p>"},{"location":"how-tos-faqs/faq-agents-managers/#how-do-i-view-the-live-transcript-of-a-conversation","title":"How do I view the live transcript of a conversation?","text":"<p>A: Click the  icon corresponding to the conversation you\u2019d like to view. In the callout that appears, click the View Conversation button to see the complete transcript; displayed in a new tab in your browser.  </p> <p></p>"},{"location":"how-tos-faqs/faq-agents-managers/#how-do-i-view-all-the-live-conversations-for-a-single-agent","title":"How do I view all the live conversations for a single agent?","text":"<p>A: You will need to define a few additional filters:   First, select Live conversations (and Wrap Up if applicable).    Click Apply to activate your selected filters.    Then, sort the conversations by Agent.    You can scroll to see all the live conversations in which any particular agent is participating.  </p> <p> </p>"},{"location":"how-tos-faqs/faq-agents-managers/#how-do-i-set-up-filters","title":"How do I set up filters?","text":"<p>A: Click Filters to define the filters you would like to apply. You can change the qualifier as necessary. For example, you can choose Between, Greater than, and Less than amongst others, when filtering on Sentiment Score. Ensure that you click the Apply button to activate your chosen filters.  </p> <p></p>"},{"location":"how-tos-faqs/how-to-find-conversations-in-the-platform/","title":"How to find conversations in the platform?","text":"<p>About this article </p> <p>Reference links to other articles are indicated but unavailable since these are sample documents.</p>"},{"location":"how-tos-faqs/how-to-find-conversations-in-the-platform/#accessing-your-conversations","title":"Accessing your conversations","text":"<ol> <li> <p>To view a list of all your past conversations, from the Left Navigation Bar, click Conversations &gt; click the Conversations sub-option.  </p> <p> </p> </li> <li> <p>A table listing various interactions along with their details are displayed. By default, conversations from the past 30 days are shown.</p> </li> <li> <p>Hover over the icon to read a summary of the corresponding conversation.  </p> <p></p> </li> </ol>"},{"location":"how-tos-faqs/how-to-find-conversations-in-the-platform/#finding-specific-conversations","title":"Finding Specific Conversations","text":""},{"location":"how-tos-faqs/how-to-find-conversations-in-the-platform/#using-the-filters-panel","title":"Using the Filters Panel","text":"<ol> <li> <p>Use the dropdown to modify the time period.  </p> <p> </p> </li> <li> <p>You can use various filters available in the Filters panel to refine and extract specific data. All filter options are organized into categories for easy identification. The Interaction Field category contains data imported from your CRM platform, such as Salesforce or Freshdesk, which you can use as filters.</p> </li> </ol> <p>INFO</p> <p>For more information on how to use filters, refer to this article.</p> <p> </p> <p>If you use a set of filters frequently, you can save them as Views using the Save button at the bottom right of the screen. This will help you to retrieve your data quickly without repeatedly configuring your filters each time.  </p> <p> </p> <p>INFO</p> <p>For more information on creating and using views, refer to this article.</p> <p>To return to the standard view where interactions of the last 30 days are displayed, from the Views dropdown, click All Interactions.  </p> <p></p>"},{"location":"how-tos-faqs/how-to-find-conversations-in-the-platform/#using-search-transcript","title":"Using Search Transcript","text":"<p>You can find conversations by entering words or phrases mentioned by the customer or agent in the Search Transcript field. For example, typing in refund in the text field and clicking Search will display all relevant conversations in which the customer or agent mentioned the word refund.</p> <p>TIP</p> <p>The search results will highlight your search term for easy reference; customer utterances are marked in blue and agent utterances in orange, making it easy to identify relevant conversations.</p> <p>INFO</p> <p>For more information on how to use Transcript Search, refer to this article.</p> <p></p>"},{"location":"how-tos-faqs/how-to-find-conversations-in-the-platform/#rearranging-table-columns","title":"Rearranging table columns","text":"<p>To rearrange or modify the columns of your table, click the Column Customizer button. Drag and drop column names (A) to rearrange them. Toggle the slider (B) to display or hide columns as needed.</p> <p>INFO</p> <p>For more information on customizing the display of the interactions table, refer to this article.</p> <p> </p>"},{"location":"how-tos-faqs/how-to-find-conversations-in-the-platform/#exporting-table-data","title":"Exporting Table Data","text":"<p>You can download your table data as a CSV file for offline analysis or sharing by clicking . Select your preferred option and click the corresponding Export button. A download link will be sent to your email address.  </p> <p></p>"},{"location":"process-tools/","title":"Tools I Use","text":"<p>Here's a list of all the tools I've used at various points of time in my tech writing journey.</p> <p>P.S.: Since every organization has its own unique set of tools, I've had to learn a few on the job; this isn't a dealbreaker since I'm a quick learner (really!) </p>"},{"location":"process-tools/#for-documentation","title":"For Documentation","text":"<p>Confluence | Zendesk Guide | Google Docs | MS Office | Robohelp</p>"},{"location":"process-tools/#for-images-and-videos","title":"For Images and Videos","text":"<p>SnagIt | Adobe Illustrator | Adobe Photoshop | Loom | Trupeer | Captivate</p>"},{"location":"process-tools/#for-digital-adoption","title":"For Digital Adoption","text":"<p>Walkme | Whatfix</p>"},{"location":"process-tools/#for-sprints-project-management","title":"For Sprints/ Project Management","text":"<p>Jira</p>"},{"location":"process-tools/#i-also-know-the-basics-of","title":"I also know the basics of","text":"<p>HTML | JSON | Markdown | DITA | XML | Github | Gitlab | VSCode</p>"},{"location":"process-tools/#ai-tools","title":"AI Tools","text":"<p>Gemini | ChatGPT Plus | Napkin.ai</p> <p></p>"},{"location":"release-notes/","title":"Release Notes samples","text":""},{"location":"release-notes/#release-notes","title":"Release Notes","text":"<p>Release notes in CMS \u2192 </p> <p>Release announcements in-app \u2192 </p> <p></p>"},{"location":"release-notes/release-note-app/","title":"Release announcements in-app","text":"<p>About this article </p> <p>This article contains image compilations of in-app release announcements I created.</p>"},{"location":"release-notes/release-note-app/#overall-release-in-app-comms","title":"Overall Release in-app comms","text":""},{"location":"release-notes/release-note-app/#feature-specific-release-in-app-comms","title":"Feature-specific Release in-app comms","text":""},{"location":"release-notes/release-note-cms/","title":"Release notes in CMS","text":"<p>About this article </p> <p>Reference links to other articles are indicated but unavailable since these are sample documents.</p>"},{"location":"release-notes/release-note-cms/#july-2024-whats-new-in-this-release","title":"(July 2024) What's new in this release","text":""},{"location":"release-notes/release-note-cms/#available-to-all","title":"\ud83d\udfe2 Available to all","text":""},{"location":"release-notes/release-note-cms/#conversations-search-enhancements","title":"Conversations Search Enhancements","text":"<p>We've launched several new enhancements to the conversation search experience, making it faster and easier to find the exact conversations you're looking for.</p> <p> Refer to this support article to learn more.  </p> <p></p> <p>Highlights:</p> <ul> <li>Smarter Multi-Keyword Search: When you search for multiple keywords (e.g., \"cancel order\"), our improved search will now find conversations where those words appear within 5 words of each other, providing more relevant results.</li> <li>Powerful Queries: Our new Advanced Search feature allows you to build complex queries using <code>AND</code>, <code>OR</code>, and <code>NOT</code> operators.</li> <li>Expanded Search Options: You can now search directly by agent name, agent email, evaluator name, or evaluator email.</li> <li>Recent Searches: Save time with Recent Searches, which automatically saves your last three queries for up to 12 hours.</li> <li>One-Click to Conversation: Searching by a Conversation ID now takes you directly to the conversation page.</li> <li>Easy Sharing: A new Copy to clipboard icon lets you easily share shortened conversation links.</li> </ul> <p>These updates, along with a redesigned list view, give you a cleaner, more efficient search experience.</p>"},{"location":"release-notes/release-note-cms/#new-executive-summary-assistant","title":"New: Executive Summary Assistant","text":"<p>We're excited to introduce Executive Summary, our newest AI Assistant designed for in-depth analysis. While our existing Search Assistant is great for quick, tactical insights, Executive Summary is built to produce an executive-grade report by turning large volumes of conversations into a strategic, long-form document.\u00a0  </p> <p>It takes a single high-level business question, such as What's driving negative CSAT over the last three months? and: - automatically breaks it down into 10\u201315 sub-questions - runs multiple deep-search queries in parallel and synthesizes the results into a detailed report. This process, optimized for depth over speed, takes approximately 5\u20137 minutes to run.</p> <p>Executive Summary delivers strategic clarity and actionable insights, making it perfect for quarterly reviews, product audits, and board-level reporting. The output includes an executive summary, thematic analysis, key metrics, and actionable recommendations. You can also re-run the same prompt over time to track key trends and measure performance month over month.</p> <p> Refer to this support article to learn more.  </p>"},{"location":"release-notes/release-note-cms/#restart-chat","title":"Restart Chat","text":"<p>The new Restart Chat feature that allows you to reopen and continue any previous conversation with an AI Assistant, regardless of when it occurred. Previously, navigating away from a conversation would end it, forcing you to start over. This new feature provides contextual continuity, allowing you to pick up exactly where you left off with any of our AI Assistants (including Search, Coach, Data Sentiment Insights, iCSAT, Resolution Insights, and Deep Research Assistants) and continue your analysis with the original context intact.</p>"},{"location":"release-notes/release-note-cms/#early-access","title":"\ud83d\udfe8 Early Access","text":"<p>Rolling out to select accounts.</p>"},{"location":"release-notes/release-note-cms/#insights-panel-on-homepage-and-email-digest-for-cx-leaders","title":"Insights panel on Homepage and Email Digest for CX Leaders","text":"<p>We've introduced customized weekly insights for leaders, delivered directly to the Homepage and via email digest. This brings the most critical data points such as key metrics in CX, QA Performance, and QA Coverage directly to users, making it faster and easier to spot trends without navigating multiple dashboards.</p> <p>A new Leader label in user profiles ensures these targeted insights reach the right people (only admins and superadmins can be set as a leader). Every Monday at 9 a.m. PST, leaders will receive a personalized email summarizing the previous week's data. For all other users, the Homepage and email digest will remain unchanged.  </p> <p></p> <p></p>"},{"location":"release-notes/release-note-cms/#bug-fixes","title":"\ud83c\udf10 Bug Fixes","text":"<p>We identified rough edges and smoothed them out to make your Acme AI experience better ! </p>"},{"location":"training-guides/","title":"Training guides","text":""},{"location":"training-guides/#internal-training","title":"Internal training","text":"<p>Image annotation training guide \u2192</p>"},{"location":"training-guides/#external-training","title":"External training","text":"<p>AI prompt training guide \u2192</p> <p></p>"},{"location":"training-guides/ai-prompt/","title":"AI prompts training guide","text":"<p>About this article </p> <p>This document is an excerpt from a specialized, targeted, 15-page Prompting guide that I created as training collateral for a product webinar. The guide was shared with 1000+ external customers (mostly in senior management and CXO positions).</p>"},{"location":"training-guides/ai-prompt/#prompting-basics","title":"Prompting Basics","text":""},{"location":"training-guides/ai-prompt/#purpose","title":"Purpose","text":"<p>We\u2019ve introduced AI Assistants to help your teams uncover insights and take faster action across customer experience challenges. However, to get the most value from AI Assistants, the quality of your prompt matters. This guide will teach you how to move from \"getting answers\" to \"getting the right answers\" \u2014 the kind you can actually act on as a manager, leader, or CXO.</p>"},{"location":"training-guides/ai-prompt/#why-prompts-matter","title":"Why prompts matter","text":"<p>Consider a prompt as the question you would ask a trusted analyst on your team. The clearer you are, the more actionable the answer becomes.</p> <ul> <li>A vague prompt: \u201cTell me about customers.\u201d \u2192 Returns broad, generic results.  </li> <li>A focused prompt: \u201cWhat are the top 5 reasons customers called about billing errors in July?\u201d \u2192 Returns specific, usable insight.</li> </ul>"},{"location":"training-guides/ai-prompt/#quick-start-guide-to-prompting","title":"Quick start guide to prompting","text":"<p>Here are a few, easy-to-follow rules to write high-quality prompts.</p> <ol> <li>Start with one clear, specific question.  </li> <li>Add timeframe, product, or channel context.  </li> <li>Use business-friendly action verbs (show, list, compare).  </li> <li>Iterate until the answer feels usable. Don\u2019t worry about always getting it right the first time.  </li> <li>Remember: The AI Assistant is only as good as the data and the prompt!</li> </ol>"},{"location":"training-guides/ai-prompt/#best-practices-of-prompt-writing","title":"Best practices of prompt writing","text":"Tip Why it Works Example Be specific Helps the Assistant target relevant conversations and extract precise insights Top reasons for returns or cancellations in last 30 days Add timelines Helps track trends and provides insights as per your time period of interest Instead of Payment issues Ask\u2192  Payment issues in the last 7 days Use metadata field names Increases accuracy by anchoring to structured fields (like agent name, team, campaign, product) Instead of  Top issues  with Northeast growth and Acme 45 Ask \u2192 Top issues  with campaign = Northeast growth and Team = Acme 45 Break down big asks Avoids vague outputs and enables deeper multi-step investigation Instead of Tell me everything about sales Ask \u2192 What are the top objections raised in sales calls"},{"location":"training-guides/ai-prompt/#prompt-templates","title":"Prompt templates","text":"<p>Use these plug-and-play templates to get started:</p> <ul> <li> <p>Trend Analysis (VOC Assistant) \u201cWhat are the top [X] reasons customers contacted support about [topic] in [timeframe]?\u201d</p> </li> <li> <p>Performance Insight (Search Assistant): \u201cShow me conversations where customers complained about [specific pain point].\u201d</p> </li> <li> <p>Resolution Focus \u201cWhich issues are most associated with [low resolution rates / repeat calls / escalations]?\u201d</p> </li> <li> <p>Sentiment Tracking \u201cWhat are the top drivers of negative sentiment in [month/quarter]?\u201d</p> </li> <li> <p>Agent Behavior \u201cProvide transcripts where agents successfully handled [objection / escalation].\u201d</p> </li> </ul>"},{"location":"training-guides/ai-prompt/#limitations","title":"Limitations","text":"<p>\u25ba Data Boundaries: AI Assistants only analyze conversations and metadata available in your Zumo AI instance. If the data isn\u2019t captured, the Assistant can\u2019t report on it.</p> <p>\u25ba Prompt Scope: Broad prompts may return generalized patterns instead of specific insights.</p> <p>\u25ba Iteration is Key: Your second or third attempt at phrasing may unlock the real insight.</p> <p>\u25ba Responses reflect patterns, not guarantees. Use AI Assistant responses for decision support, not absolute truth.</p>"},{"location":"training-guides/ai-prompt/#troubleshooting-your-prompts","title":"Troubleshooting your prompts","text":"Problem Likely Cause How to Fix It Answer feels too generic Prompt was too broad or vague Add timeframe, product, issue type, or metric. No clear answer returned Data doesn\u2019t exist or prompt too vague Check if the data exists by asking \u201cWhat metadata fields do you have available to analyze?\u201d. Rephrase your subsequent prompts accordingly or check the \u2018Thought Process\u2019 Output not in desired format Response style is not specified Add \u201cFormat the response as a table/concise summary/professional report\u201d Insight feels disconnected from action No action items requested in the prompt Re-prompt with \u201c\u2026and suggest 2 actions managers can take.\u201d"},{"location":"training-guides/annotation/","title":"Image annotation training guide","text":"<p>About this article </p> <p>This document was used by the AI/ML Engineers to train the in-house team of 50+ graphic designers for data annotation during the development of AI-based image-processing APIs.</p>"},{"location":"training-guides/annotation/#introduction-and-objective","title":"Introduction and Objective","text":"<p>The Image Processing team is working on automatic removal of the background from lineart images uploaded by the customer. In many cases, the output image generated after removing the background may have some regions missing since they are of the same color as the background, or it may have unwanted changes such as pixelation, fuzziness, or irregular patches that were not present in the original image. </p> <p>The objective of this annotation project is to compare the original input image and the output image and identify whether the background removal process was carried out correctly or not.</p>"},{"location":"training-guides/annotation/#task","title":"Task","text":"<p>In this project, you are required to categorize the image into three types: Acceptable, Not Acceptable, and Not Sure.    </p> <p>Identifying ACCEPTABLE images: Generally, the background removal result looks good, appearance to the original artwork is preserved and there are very little or no artifacts. Also, holes in the artwork are filled in only where necessary.    </p> <p>Identifying UNACCEPTABLE images: The appearance to the original artwork is not preserved and major artifacts are present. Important holes are not filled in, and some holes are filled in where they shouldn\u2019t be. Also, there would be partially removed gradients/drop shadows that weren\u2019t there in the original image.</p> <p>The following images show the categorization:</p> <p>NOTE</p> <p>The red background in the columns are for illustration purposes only, and have been used in order to view the background removal results more clearly.</p> <p></p> <p></p> <p>Identifying NOT SURE images: The appearance to the original artwork is preserved, and the artifacts aren\u2019t bad \u2013 this categorization is for images that are acceptable in principle, but could be better, or it is unclear whether the final result will be acceptable for a customer.   </p> <p></p>"},{"location":"training-guides/annotation/#how-to-launch-and-use-the-annotation-app","title":"How to Launch and Use the Annotation App","text":"<ul> <li> <p>Click https://annotation-review.pxxnss.cxxxznss.io/ </p> </li> <li> <p>Log in using your Auth0 credentials</p> </li> <li> <p>The Annotation workspace is displayed:   </p> </li> </ul> <p></p> <ol> <li>User ID Display</li> <li>Check in time</li> <li>Click Get to fetch a request</li> <li>Click Pause if you require to temporarily stop processing your requests. </li> <li>Select an option from the list and click Done. </li> <li>Click Resume to continue processing requests.</li> </ol>"},{"location":"training-guides/annotation/#how-to-annotate-an-image","title":"How to Annotate an Image","text":"<p>Step I</p> <p>Click Get to fetch a request. The artwork is displayed in the workspace.    </p> <p>(1) Displays the number of requests in the queue    </p> <p>(2) Displays the request ID and processing time for the artwork currently open in the workspace.    </p> <ul> <li>Input: Displays the original artwork uploaded by the customer.    </li> <li>Output: Displays the artwork with the background removed.    </li> </ul> <p>The output image is placed on an animated background whose color is constantly changing. This is done to help you to easily spot any problems in the output image. </p> <p>Use the Color option (3) if you want to apply a specific background color to the image in order to identify any image issues. </p> <p></p> <p>Step II</p> <p>In (4), mark one of the following options for the output image:</p> <ul> <li> <p>Acceptable: When the artwork is clean, distinct, and no irregular patches are visible.</p> </li> <li> <p>Not Sure: When the artwork appears clear but there may be some fuzzy or pixelated patches within the image. </p> </li> <li> <p>Not Acceptable: When the artwork is pixelated, fuzzy, has some missing regions, or background patches are visible within the image.</p> </li> </ul> <p>Step III</p> <p>Click Release (5) to complete the current request and move it to further processing.</p>"},{"location":"ui-feature/","title":"UI documentation","text":""},{"location":"ui-feature/#ui","title":"UI","text":"<p>(Natural language) Layout of the Conversations Details page \u2192</p> <p>(Chatbot-compatible) Homepage - Introduction \u2192 </p>"},{"location":"ui-feature/#feature","title":"Feature","text":"<p>(Natural language) AgentGPT \u2192 </p> <p>KEEP IN MIND</p> <p>Natural language support articles prioritize natural language, flow, and formatting so they can be read and understood easily by humans.   Chatbot-compatible support articles are specifically structured and formatted for automated systems to easily ingest and process.    The two formats have different goals and are often used together, with chatbots directing users to the more detailed, human-friendly content.</p> <p></p>"},{"location":"ui-feature/agentgpt/","title":"AgentGPT","text":"<p>About this article </p> <p>This is a \"natural language\" help article structure.</p>"},{"location":"ui-feature/agentgpt/#overview","title":"Overview","text":"<p>Contact center agents have several challenges irrespective of the industry to which they belong, the common ones being:</p> <ul> <li>New agent struggles: It's tough for new agents to learn everything quickly.</li> <li>Knowledge pressure: Expected to be experts on all topics.</li> <li>Inconsistence in answers: Agents have varying levels of expertise and give different answers, which can frustrate customers.</li> <li>Fear of asking for help because they\u2019re afraid to look dumb.</li> <li>Customer frustration: Putting customers on hold to search for information can lead to further frustration.</li> <li>Note-taking struggles: Balancing note-taking with maintaining focus on the conversation.</li> <li>Overwhelming workload: High call volume, limited support, and emotional exhaustion.</li> </ul> <p>To address these challenges, Zumo AI has designed AgentGPT and Suggested Queries, AI-powered solutions that empower your agents to deliver exceptional customer service.</p>"},{"location":"ui-feature/agentgpt/#agentgpt","title":"AgentGPT","text":"<p>AgentGPT acts like a virtual assistant, providing agents with the right information when you need it. If you\u2019re stuck or need help in real time, ask AgentGPT! Get instant answers and expert suggestions without the awkward wait or fear of asking \"silly\" questions. Instead of sifting through long articles, you\u2019ll receive only the most relevant information from across multiple articles, saving your time and improving efficiency.  </p> <p></p>"},{"location":"ui-feature/agentgpt/#benefits-of-agentgpt","title":"Benefits of AgentGPT","text":"<p>\u25b6 Reduced AHT: By providing quick access to information, AgentGPT helps agents resolve issues more efficiently, leading to lower AHT.</p> <p>\u25b6 Improved CSAT: Empowered agents can provide faster and more accurate solutions, leading to happier customers and higher CSAT scores.</p> <p>\u25b6 Increased Agent Confidence: AgentGPT provides agents with the support they need to feel confident and knowledgeable, reducing stress and improving job satisfaction.</p> <p></p>"},{"location":"ui-feature/agentgpt/#suggested-queries","title":"Suggested Queries","text":"<p>We recognize that managing a conversation with both a customer and a bot simultaneously can be challenging for agents. Suggested Queries simplifies the use of AgentGPT, making it easier for agents to access their knowledge base. Here's how it works:</p> <ul> <li>Real-time Conversation Analysis: Zumo AI's models analyze the conversation between the agent and the customer in real-time, identifying the customer's intent and the topic of discussion.</li> <li>Intelligent Query Suggestions: Based on the conversation analysis, Suggested Queries presents the agent with a list of relevant queries that are likely to yield helpful information from AgentGPT.</li> <li>Help with probing questions: Sometimes, it\u2019s not clear what exactly a customer is asking about. In this case, Suggested Queries could help you formulate questions to ask the customer in order to understand what their problem actually is.</li> <li>Agent Selection and Information Retrieval: Finally, simply select the most appropriate query from the list, and AgentGPT instantly retrieves the relevant information from across your knowledge base.</li> </ul>"},{"location":"ui-feature/agentgpt/#benefits-of-suggested-queries","title":"Benefits of Suggested Queries","text":"<p>\u25b6 Less thinking, more doing: You don't have to figure out what questions to ask the bot during a call. Suggested Queries displays a variety of relevant questions that you can simply click for an answer; so, you can focus on the customer.</p> <p>\u25b6 Faster answers: Get the information you need quickly, so you can help customers faster and spend less time on each call.</p> <p>\u25b6 Accurate Answers: Suggested Queries ensures that you receive the most relevant information, so you can give accurate and consistent responses during your interactions.</p> <p></p>"},{"location":"ui-feature/agentgpt/#example-agentgpt-and-suggested-queries-in-action","title":"Example - AgentGPT and Suggested Queries in action","text":"<p>Imagine a customer calls in to inquire about returning a product. As the agent converses with the customer, Suggested Queries might offer options such as: Return shipping label request, How to initiate a return, and Return policy for online purchases.  </p> <p></p> <p>The agent can then select the most relevant query, and AgentGPT will provide the necessary information from the knowledge base, allowing the agent to quickly and accurately address the customer's request.</p> <p></p> <p>INFO</p> <p>Both AgentGPT and Suggested Queries work in the background, providing support without overwhelming agents with information.</p>"},{"location":"ui-feature/homepage-introduction/","title":"Homepage Introduction","text":"<p>About this article </p> <p>This is a \"chatbot-compatible\" help article structure.</p> <p>Reference links to other articles are indicated but unavailable since these are sample documents.</p>"},{"location":"ui-feature/homepage-introduction/#short-description","title":"Short Description","text":"<p>The Homepage is your daily dashboard. It displays goals, reminders, key conversations, scorecards, charts, and filters tailored to your role. Use it to track progress, review tasks, and refresh data with one click.</p> <p></p> <p>The Homepage acts like your control panel; a quick snapshot of everything that matters right now. At a glance, you can see goals, reminders, team conversations, metrics, and charts. Why it matters: Unlike detailed reports (historical deep dives), the Homepage gives you a live feed of activity to guide your daily and weekly decisions.</p>"},{"location":"ui-feature/homepage-introduction/#sections-of-the-homepage","title":"Sections of the Homepage","text":""},{"location":"ui-feature/homepage-introduction/#goals-widget","title":"Goals Widget","text":"<p>Purpose: Monitor progress toward performance objectives.      </p> <p>The Goals widget is dynamic and displays goals based on your user role:</p> <ul> <li>Admins / Super Admins: Organization-wide goals and agent-level tracking.  </li> <li>Managers / QA Auditors: Weekly team targets.  </li> <li>Agents: Only their individual goals.</li> </ul> <p>Tip</p> <p>Check your Goals widget daily to know if you\u2019re on track.</p>"},{"location":"ui-feature/homepage-introduction/#reminders-widget","title":"Reminders Widget","text":"<p>Purpose: Displays pending tasks from the last 30 days.      </p> <p>The Reminders widget displays evaluation, calibration, and coaching tasks based on your user role:</p> <ul> <li>Evaluations </li> <li>Agents: Conversations to review or disputed evaluations requiring action.  </li> <li>QA Auditors: Assigned evaluations, pending evaluations, or disputes to resolve.  </li> <li>Calibrations: Details of calibration sessions where you are a moderator or participant.  </li> <li>Coaching: Outstanding items from coaching sessions.</li> </ul> <p>Available actions: Click any item to open it directly.</p>"},{"location":"ui-feature/homepage-introduction/#key-conversations-to-review","title":"Key Conversations to Review","text":"<p>Purpose: Highlights conversations from the past 7 days that require attention.    </p> <p>The interactions displayed here are based on their relevance to the user role: </p> <ul> <li>Agents: Conversations to learn from and improve performance.  </li> <li>Managers / QA Auditors: Conversations with strong positive or negative sentiment handled by their team.</li> </ul>"},{"location":"ui-feature/homepage-introduction/#key-metrics","title":"Key Metrics","text":"<p> Purpose: Quick scorecards with performance indicators.      </p> <p>Role-specific view:    </p> <ul> <li>Super Admins/ Admins / Managers / QA Auditors will see 6 scorecards.  </li> <li>Agents will see 3 scorecards.</li> </ul> <p>Note</p> <p>If you click into a scorecard, Homepage filters do not carry over.</p>"},{"location":"ui-feature/homepage-introduction/#my-charts","title":"My Charts","text":"<p>Purpose: Visual summaries of data relevant to your role.     </p> <p>You can carry out the following actions on any chart:   </p> <ul> <li>Toggle labels or adjust time granularity.  </li> <li>Expand charts for detailed viewing.  </li> <li>Click chart hyperlinks to explore deeper reports.  </li> <li>Download custom charts if a  icon is shown.</li> </ul> <p></p>"},{"location":"ui-feature/homepage-introduction/#homepage-filters","title":"Homepage filters","text":"<p> Default: Last 30 days of data. Customization: Adjust filters to focus on specific time ranges.</p> <p>Important</p> <p>Filters do not apply to the Reminders or Key conversations to review widgets.</p>"},{"location":"ui-feature/homepage-introduction/#load-data","title":"Load Data","text":"<p>Available actions: Click Load Data to refresh all Homepage widgets with the most up-to-date information.</p> <p>Related Articles</p> <p>How to Customize the Homepage</p>"},{"location":"ui-feature/layout-of-the-conversation-details-page/","title":"Layout of the Conversation Details page","text":"<p>About this article </p> <p>This is a \"natural language\" help article structure.</p> <p>Reference links to other articles are indicated but unavailable since these are sample documents.</p>"},{"location":"ui-feature/layout-of-the-conversation-details-page/#overall-layout","title":"Overall Layout","text":"<p>(A) Conversation ID</p> <p>(B) Navigation to help you move between the previous and next evaluation.</p> <p>(C) Evaluation scores such as QA Score (Manual), Instascore (Automated QA), CSAT, and Sentiment Score</p> <p>(D) Rubric</p> <p>INFO</p> <p>Refer to this article for a detailed description of the Rubric section.</p> <p>(E) Key Events tab lists metric tags and conversation tags relevant to the conversation along with the corresponding audio snippet.</p> <p>Clickto expand and listen to the snippet.  </p> <p>Assists and Flags tab lists the assist cards and flags generated during the conversation.</p> <p>TIP</p> <p>Use the dropdown filters in the Key Events tab to conveniently find the desired tag.</p> <p>(F) Transcripts of the conversation</p> <p>(G) Search Transcript; use this field to look for specific customer or agent utterances</p> <p>(H) Audio and video recordings. Use the controls to select a specific point or duration that you want to listen/view.</p> <p>NOTE</p> <p>Audio files are generated only for calls.</p> <p>(I) AI- generated Conversation Summary</p> <p>(J) Share; use this button when you want to send the current conversation to other users.</p> <p>(K) Add to dropdown with: - Add to Library: Allows you to add the current conversation to a library. - Add to Coaching: Allows to you add the current conversation to a new or existing coaching session.</p> <p>INFO</p> <p>Refer to this page for more information on coaching sessions. Refer to this page for more information on libraries.</p> <p>(L) Metadata button for viewing additional fields</p>"},{"location":"ui-feature/layout-of-the-conversation-details-page/#rubric-panel","title":"Rubric panel","text":"<p>On opening a conversation, you can view the following:  </p> <p>(1) Name of the rubric used to evaluate the conversation - The  icon indicates automatic save i.e., the options you select or text you enter on the page are being saved every few seconds.  </p> <p></p> <p>(2) The status of the evaluation. This is dynamic and can display any one of the following statuses based on the actions you or an agent have taken: Acceptance pending: The agent is yet to view and accept the evaluation Accepted: The agent has viewed and fully accepted the evaluation Disputed: The agent, his/ her Team Manager, or Admin has disputed one or more questions in the evaluation  </p> <p>(3) The sections of the rubric are displayed as tabs. You can click the tab name to directly move to a particular section.</p> <p>(4) Click to view the assignment rules of the rubric.</p> <p>Click \u22ee for more options. Use the Change Rubric option if you want to replace the rubric used for evaluating the conversation.</p> <p>(5) Questions contained in the respective section, based on which you need to evaluate the conversation. Use the dropdown to select your answers.</p>"}]}